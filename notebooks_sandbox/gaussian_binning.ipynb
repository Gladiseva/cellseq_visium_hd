{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we start with 3204769 transcripts, all transcripts were assigned to one of the polygons\n",
    "from xenium we know that they detected 216323 cells, we detected 167780  or 107806 after processing (including filtering of 26272, because polygon was not uniquely mapped to a cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 12:23:22.153814: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('utils')\n",
    "import os\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import anndata\n",
    "import geopandas as gpd\n",
    "\n",
    "from stardist.models import StarDist2D\n",
    "\n",
    "from tifffile import imread, imwrite\n",
    "from csbdeep.utils import normalize\n",
    "from shapely.geometry import Polygon, Point\n",
    "from scipy import sparse\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_1</td>\n",
       "      <td>POLYGON ((2116.705 352.000, 2115.873 353.964, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_2</td>\n",
       "      <td>POLYGON ((2988.448 2654.000, 2988.674 2656.521...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_3</td>\n",
       "      <td>POLYGON ((2971.104 3174.000, 2970.065 3175.604...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_4</td>\n",
       "      <td>POLYGON ((3030.417 3494.000, 3029.509 3496.289...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_5</td>\n",
       "      <td>POLYGON ((3022.134 3302.000, 3021.518 3304.291...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216348</th>\n",
       "      <td>ID_216349</td>\n",
       "      <td>POLYGON ((27908.171 21580.000, 27907.469 21581...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216349</th>\n",
       "      <td>ID_216350</td>\n",
       "      <td>POLYGON ((28906.574 21238.000, 28907.262 21239...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216350</th>\n",
       "      <td>ID_216351</td>\n",
       "      <td>POLYGON ((27663.020 21014.000, 27662.251 21015...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216351</th>\n",
       "      <td>ID_216352</td>\n",
       "      <td>POLYGON ((28863.908 20810.000, 28864.447 20810...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216352</th>\n",
       "      <td>ID_216353</td>\n",
       "      <td>POLYGON ((28737.483 22054.000, 28737.127 22055...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>216353 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                           geometry\n",
       "0            ID_1  POLYGON ((2116.705 352.000, 2115.873 353.964, ...\n",
       "1            ID_2  POLYGON ((2988.448 2654.000, 2988.674 2656.521...\n",
       "2            ID_3  POLYGON ((2971.104 3174.000, 2970.065 3175.604...\n",
       "3            ID_4  POLYGON ((3030.417 3494.000, 3029.509 3496.289...\n",
       "4            ID_5  POLYGON ((3022.134 3302.000, 3021.518 3304.291...\n",
       "...           ...                                                ...\n",
       "216348  ID_216349  POLYGON ((27908.171 21580.000, 27907.469 21581...\n",
       "216349  ID_216350  POLYGON ((28906.574 21238.000, 28907.262 21239...\n",
       "216350  ID_216351  POLYGON ((27663.020 21014.000, 27662.251 21015...\n",
       "216351  ID_216352  POLYGON ((28863.908 20810.000, 28864.447 20810...\n",
       "216352  ID_216353  POLYGON ((28737.483 22054.000, 28737.127 22055...\n",
       "\n",
       "[216353 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf = gpd.read_file('../data/xenium/stardist_all.geojson')\n",
    "gdf['id'] = [f\"ID_{i+1}\" for i, _ in enumerate(gdf.index)]\n",
    "gdf.crs = None\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf['centroid'] = gdf.geometry.centroid\n",
    "\n",
    "# If you want to extract the x and y coordinates of the centroids\n",
    "gdf['centroid_x'] = gdf.centroid.x\n",
    "gdf['centroid_y'] = gdf.centroid.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 3204769 × 541\n",
       "    obs: 'sample', 'start_x', 'start_y', 'end_x', 'end_y', 'center_x', 'center_y', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes'\n",
       "    var: 'transcript_ids', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts'\n",
       "    obsm: 'end_x', 'end_y', 'spatial', 'start_x', 'start_y'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import scanpy as sc\n",
    "adata = sc.read_h5ad('../data/xenium/outs/transcripts_anndata_final.h5ad')\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3361359336.py (39): \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import pandas as pd\n",
    "\n",
    "def assign_transcripts_to_polygons(transcripts_df, gdf, std_dev):\n",
    "    # Convert centroids to numpy array\n",
    "    polygon_centroids = np.array([(p.x, p.y) for p in gdf['centroid']])\n",
    "    \n",
    "    # Extract transcript coordinates and counts\n",
    "    transcript_coords = transcripts_df[['center_x', 'center_y']].values\n",
    "    transcript_counts = transcripts_df['total_counts'].values\n",
    "    \n",
    "    # Compute squared differences and distances\n",
    "    diff = transcript_coords[:, np.newaxis, :] - polygon_centroids[np.newaxis, :, :]\n",
    "    squared_diff = np.sum(diff ** 2, axis=2)\n",
    "    distances = np.sqrt(squared_diff)\n",
    "    \n",
    "    # Calculate probabilities\n",
    "    probabilities = norm.pdf(distances, scale=std_dev)\n",
    "    \n",
    "    # Weight probabilities by transcript counts\n",
    "    weighted_probabilities = probabilities * transcript_counts[:, np.newaxis]\n",
    "    \n",
    "    # Normalize the weighted probabilities to sum to 1\n",
    "    row_sums = np.sum(weighted_probabilities, axis=1, keepdims=True)\n",
    "    normalized_probabilities = weighted_probabilities / row_sums\n",
    "    \n",
    "    # Create a cumulative probability array for each transcript\n",
    "    cumulative_probabilities = np.cumsum(normalized_probabilities, axis=1)\n",
    "    \n",
    "    # Generate random numbers for sampling\n",
    "    random_values = np.random.rand(len(transcripts_df))\n",
    "    \n",
    "    # Determine chosen polygon indices\n",
    "    chosen_polygon_indices = np.argmax(cumulative_probabilities >= random_values[:, np.newaxis], axis=1)\n",
    "    \n",
    "    # Assign polygon IDs to transcripts\n",
    "    transcript_to_polygon_id = gdf['id'].values[chosen_polygon_indices]\n",
    "    transcripts_df['assigned_polygon_id'] = transcript_to_polygon_id\n",
    "    \n",
    "    return transcripts_df\n",
    "\n",
    "# Parameters\n",
    "std_dev = 100 / 2\n",
    "\n",
    "# Example usage\n",
    "transcripts_with_assignments = assign_transcripts_to_polygons(adata.obs.iloc[:1000], gdf, std_dev)\n",
    "\n",
    "# Save to CSV\n",
    "csv_file_path = 'transcripts_with_assignments.csv'\n",
    "transcripts_with_assignments.to_csv(csv_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole polygon geometry, not only centroids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(43811) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(43812) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(43814) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(43815) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(43816) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(43817) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(43818) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(43819) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Process SpawnPoolWorker-17:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lollijagladiseva/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/lollijagladiseva/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/lollijagladiseva/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/lollijagladiseva/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'assign_transcripts_chunk' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-18:\n",
      "Traceback (most recent call last):\n",
      "python(43820) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "  File \"/Users/lollijagladiseva/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/lollijagladiseva/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/lollijagladiseva/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/lollijagladiseva/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'assign_transcripts_chunk' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-19:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lollijagladiseva/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/lollijagladiseva/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/lollijagladiseva/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/lollijagladiseva/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'assign_transcripts_chunk' on <module '__main__' (built-in)>\n",
      "python(43821) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Process SpawnPoolWorker-20:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lollijagladiseva/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/lollijagladiseva/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/lollijagladiseva/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/lollijagladiseva/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'assign_transcripts_chunk' on <module '__main__' (built-in)>\n",
      "python(43822) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Process SpawnPoolWorker-21:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lollijagladiseva/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/lollijagladiseva/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/lollijagladiseva/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/lollijagladiseva/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'assign_transcripts_chunk' on <module '__main__' (built-in)>\n",
      "python(43823) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Process SpawnPoolWorker-22:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lollijagladiseva/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/lollijagladiseva/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/lollijagladiseva/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/lollijagladiseva/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'assign_transcripts_chunk' on <module '__main__' (built-in)>\n",
      "python(43824) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Process SpawnPoolWorker-23:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lollijagladiseva/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/lollijagladiseva/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/lollijagladiseva/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/lollijagladiseva/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'assign_transcripts_chunk' on <module '__main__' (built-in)>\n",
      "python(43825) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Process SpawnPoolWorker-24:\n",
      "Traceback (most recent call last):\n",
      "python(43826) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "  File \"/Users/lollijagladiseva/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/lollijagladiseva/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/lollijagladiseva/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/lollijagladiseva/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'assign_transcripts_chunk' on <module '__main__' (built-in)>\n",
      "python(43827) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m std_dev \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m transcripts_with_assignments \u001b[38;5;241m=\u001b[39m assign_transcripts_to_polygons(adata\u001b[38;5;241m.\u001b[39mobs\u001b[38;5;241m.\u001b[39miloc[:\u001b[38;5;241m1000\u001b[39m], gdf, std_dev)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Save to CSV\u001b[39;00m\n\u001b[1;32m     69\u001b[0m csv_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranscripts_with_assignments.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[19], line 53\u001b[0m, in \u001b[0;36massign_transcripts_to_polygons\u001b[0;34m(transcripts_df, gdf, std_dev)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Use multiprocessing Pool to process chunks in parallel\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(processes\u001b[38;5;241m=\u001b[39mnum_chunks) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m---> 53\u001b[0m     results \u001b[38;5;241m=\u001b[39m pool\u001b[38;5;241m.\u001b[39mstarmap(assign_transcripts_chunk, \n\u001b[1;32m     54\u001b[0m                            [(chunk, polygon_centroids, gdf_ids, std_dev) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m transcript_chunks])\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Combine the results into a single DataFrame\u001b[39;00m\n\u001b[1;32m     57\u001b[0m transcripts_with_assignments \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(results, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/pool.py:375\u001b[0m, in \u001b[0;36mPool.starmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstarmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    370\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m    Like `map()` method but the elements of the `iterable` are expected to\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m    be iterables as well and will be unpacked as arguments. Hence\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    `func` and (a, b) becomes func(a, b).\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_async(func, iterable, starmapstar, chunksize)\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event\u001b[38;5;241m.\u001b[39mwait(timeout)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/threading.py:622\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    620\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 622\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "def assign_transcripts_chunk(transcripts_chunk, polygon_centroids, gdf_ids, std_dev):\n",
    "    transcript_coords = transcripts_chunk[['center_x', 'center_y']].values\n",
    "    transcript_counts = transcripts_chunk['total_counts'].values\n",
    "    \n",
    "    # Compute squared differences and distances\n",
    "    diff = transcript_coords[:, np.newaxis, :] - polygon_centroids[np.newaxis, :, :]\n",
    "    squared_diff = np.sum(diff ** 2, axis=2)\n",
    "    distances = np.sqrt(squared_diff)\n",
    "    \n",
    "    # Calculate probabilities\n",
    "    probabilities = norm.pdf(distances, scale=std_dev)\n",
    "    \n",
    "    # Weight probabilities by transcript counts\n",
    "    weighted_probabilities = probabilities * transcript_counts[:, np.newaxis]\n",
    "    \n",
    "    # Normalize the weighted probabilities to sum to 1\n",
    "    row_sums = np.sum(weighted_probabilities, axis=1, keepdims=True)\n",
    "    normalized_probabilities = weighted_probabilities / row_sums\n",
    "    \n",
    "    # Create a cumulative probability array for each transcript\n",
    "    cumulative_probabilities = np.cumsum(normalized_probabilities, axis=1)\n",
    "    \n",
    "    # Generate random numbers for sampling\n",
    "    random_values = np.random.rand(len(transcripts_chunk))\n",
    "    \n",
    "    # Determine chosen polygon indices\n",
    "    chosen_polygon_indices = np.argmax(cumulative_probabilities >= random_values[:, np.newaxis], axis=1)\n",
    "    \n",
    "    # Assign polygon IDs to transcripts\n",
    "    transcripts_chunk['assigned_polygon_id'] = gdf_ids[chosen_polygon_indices]\n",
    "    \n",
    "    return transcripts_chunk\n",
    "\n",
    "def assign_transcripts_to_polygons(transcripts_df, gdf, std_dev):\n",
    "    # Convert centroids to numpy array\n",
    "    polygon_centroids = np.array([(p.x, p.y) for p in gdf['centroid']])\n",
    "    \n",
    "    # Get the polygon IDs\n",
    "    gdf_ids = gdf['id'].values\n",
    "    \n",
    "    # Split transcripts_df into chunks for parallel processing\n",
    "    num_chunks = cpu_count()\n",
    "    chunk_size = len(transcripts_df) // num_chunks\n",
    "    transcript_chunks = [transcripts_df.iloc[i*chunk_size:(i+1)*chunk_size] for i in range(num_chunks)]\n",
    "    \n",
    "    # Use multiprocessing Pool to process chunks in parallel\n",
    "    with Pool(processes=num_chunks) as pool:\n",
    "        results = pool.starmap(assign_transcripts_chunk, \n",
    "                               [(chunk, polygon_centroids, gdf_ids, std_dev) for chunk in transcript_chunks])\n",
    "    \n",
    "    # Combine the results into a single DataFrame\n",
    "    transcripts_with_assignments = pd.concat(results, ignore_index=True)\n",
    "    \n",
    "    return transcripts_with_assignments\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters\n",
    "    std_dev = 100 / 2\n",
    "\n",
    "    # Example usage\n",
    "    transcripts_with_assignments = assign_transcripts_to_polygons(adata.obs.iloc[:1000], gdf, std_dev)\n",
    "\n",
    "    # Save to CSV\n",
    "    csv_file_path = 'transcripts_with_assignments.csv'\n",
    "    transcripts_with_assignments.to_csv(csv_file_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
